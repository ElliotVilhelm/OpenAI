{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning with OpenAI gym "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Gym\n",
    "OpenAI is a non-profit AI research company. OpenAI's Python module \"gym\" provides us with many environments with which we can experiment and solve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPole \n",
    "- A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. \n",
    "\n",
    "- The system is controlled by applying a force of +1 or -1 to the cart. \n",
    "\n",
    "- The pendulum starts upright, and the goal is to prevent it from falling over. \n",
    "\n",
    "- A reward of +1 is provided for every timestep that the pole remains upright. \n",
    "\n",
    "- The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Example\n",
    "- In the cell below the environment is run for 1000 steps, at each step a random decision is made, move left or move right\n",
    "\n",
    "- The env.step() method takes an action as an input and outputs four variables, observation, reward, done, info.\n",
    "- Observation will describe our environment, the reward will represent the reward earned at the time step, done will decribes if the trial is over, meaning we have won or lost, and info will be ntohing in this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "for ep in range(10):\n",
    "    env.reset()\n",
    "    for _ in range(150):\n",
    "        env.render()\n",
    "        obv, reward, done, info = env.step(env.action_space.sample()) # take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Space & Observation Space\n",
    "\n",
    "One of the great things about OpenAI gym is that the environments are set up with a given action_space and observation_space. Here they are defined as follows.\n",
    "\n",
    "### Action Space \n",
    "- Left given as -1    (move left in x direction)\n",
    "- Right given as +1   (move right in y direction)\n",
    "* Our Action space is \"discrete\"\n",
    "\n",
    "\n",
    "\n",
    "### Observation Space\n",
    "#### ( X, X', θ, θ')\n",
    "- X = X postion of cart\n",
    "- X' = X speed of cart\n",
    "- θ = angle of pole with cart\n",
    "- θ' = angular veloctiy of pole with cart\n",
    "* Our Observation space is continuous. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space:  Discrete(2)\n",
      "Observation Space:  Box(4,)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "print (\"Action Space: \", env.action_space)\n",
    "print(\"Observation Space: \", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Loop\n",
    "\n",
    "![title](rlloop2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The environment is described by our observation space. Our agent will be given a state which will be composed of the observation space and the reward.**\n",
    "\n",
    "1. **The agent will make a decision of which action to take at the given state and reward**\n",
    "2. **The action will be applied to the environment changing the state of the environment**\n",
    "3. **The new state and reward will be provided to the agent from which it will \"learn\"**\n",
    "4. **Repeat!**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning \n",
    "The Agent given a state must have a policy to decide on which action to take. The policy we will use is Q learning.\n",
    "\n",
    "### The Bellman Equation\n",
    "the expected long-term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state.\n",
    "\n",
    "![title](qlearn.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The discount factor \"γ\" will determine how much future rewards are worth\n",
    "- The learning rate \"α\" represents how much new information will overite old information, range [0,1]. \n",
    "\n",
    "We are using Q learning to find the best action-selection policy. We will learn an action-value function which will represets the value of an action at a state. \n",
    "\n",
    "In this demo we will implement a lookup-table version of the algorithm. Initially our agent will make random decisions and gradually shift to makign its decisions off the Q table. The probability of making a random decision will be represented by the \"Exploration Rate\" which will decrease over time\n",
    "\n",
    "The lookup table, Q table, will be a matrix \"state x action\" matrix, We will have 162 states after we discretize the observation space below. We will have two actions at any given state thus out matrix will be 162 x 2. \n",
    "\n",
    "| Q Table   | Action 1: Left | Action 2: Right |\n",
    "|-----------|----------------|-----------------|\n",
    "| State 1   | 0.1            | 0.3             |\n",
    "| State 2   | -.4            | .7              |\n",
    "| State 3   | 1.3            | .3              |\n",
    "| ..        |                |                 |\n",
    "| ..        |                |                 |\n",
    "| State 161 | 2.2            | .9              |\n",
    "| State 162 | 1.2            | 1.4             |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Creating a discrete observation space\n",
    "#### ( X, X', θ, θ') ⊆ ℝ \n",
    "The set of all real numbers is an uncountably infinite set. Between 0 and 1 there are infinetly many numbers, ie: .1, .2, .3 .312342342 ....\n",
    "\n",
    "There are infitely many states and we cannot generate an infinetly large table, thus to apply this method we must generate a discrete number of states  from our infinetly many observations\n",
    "\n",
    "We will divide each variables range into a fintie number of spaces. \n",
    "\n",
    "For example, take the range **[0,1]**, although there are infitely many numbers between 0,1 we can split the range into any number of chunks.\n",
    "\n",
    "**Such as { [0, 0.25], [0.25, 0.5], [0.5, 0.75], [0.75, 1]**\n",
    "\n",
    "Now there are only four states between 0 and 1.\n",
    "We will perform a similar operation on our sample space in Python below\n",
    "\n",
    "Lets call out staes \"boxes\", \n",
    "#### Splits = (3,3,6,3)\n",
    "Our X will be split into 3 boxes, X' into 3 boxes, θ into 6 boxes, θ' into 3 boxes\n",
    "#### This yeilds 162 boxes = 3*3*6*3\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input a cartPole observation state\n",
    "# A box number ranging from 0-162\n",
    "def get_Box(obv):\n",
    "\tx, x_dot, theta, theta_dot = obv\n",
    "    \n",
    "\tif x < -.8:\n",
    "\t\tbox_number = 0\n",
    "\telif x < .8:\n",
    "\t\tbox_number = 1\n",
    "\telse:\n",
    "\t\tbox_number = 2\n",
    "\n",
    "\tif x_dot < -.5:\n",
    "\t\tpass\n",
    "\telif x_dot < .5:\n",
    "\t\tbox_number += 3\n",
    "\telse:\n",
    "\t\tbox_number += 6\n",
    "\n",
    "\tif theta < np.radians(-12):\n",
    "\t\tpass\n",
    "\telif theta < np.radians(-1.5):\n",
    "\t\tbox_number += 9\n",
    "\telif theta < np.radians(0):\n",
    "\t\tbox_number += 18\n",
    "\telif theta < np.radians(1.5):\n",
    "\t\tbox_number += 27\n",
    "\telif theta < np.radians(12):\n",
    "\t\tbox_number += 36\n",
    "\telse:\n",
    "\t\tbox_number += 45\n",
    "\n",
    "\tif theta_dot < np.radians(-50):\n",
    "\t\tpass\n",
    "\telif theta_dot < np.radians(50):\n",
    "\t\tbox_number += 54\n",
    "\telse:\n",
    "\t\tbox_number += 108\n",
    "\n",
    "\treturn box_number\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "\n",
    "##### Explore Rate\n",
    "We will need a few functions to modify the learning rate and the exploration rate. The exploration rate decides wether we make a random move or we choose based off of the Q table. The value will start high and grow smaller over time as our Q table begins to more accurately reflect the action-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Rate Decay Function, Converges to MIN_EXPLORE_RATE\n",
    "def update_explore_rate(episode):\n",
    "\treturn max(MIN_EXPLORE_RATE, min(1, 1.0 - np.log10((episode + 1) / 25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Learning Rate\n",
    "As we learn the right Q values to solve the environment we want to reduce the rate at which we over write them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Rate Decay Function, Converges to MIN_LEARNING_RATE\n",
    "def update_learning_rate(episode):\n",
    "\treturn max(MIN_LEARNING_RATE, (min(0.5, 1.0 - np.log10((episode + 1) / 50))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update Action\n",
    "This function will take the state and exploration rate as an input. It will decide with a probability dependent on the exploration rate wether to take a random action or to take the action with the maximum value on the Q table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_action(state, explore_rate):\n",
    "\tif random.random() < explore_rate:\n",
    "\t\treturn env.action_space.sample()\n",
    "\telse:\n",
    "\t\treturn np.argmax(Q[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learn\n",
    "We are now ready to define the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learn():\n",
    "\ttotal_reward = 0\n",
    "\ttotal_completions = 0\n",
    "\texplore_rate = update_explore_rate(0)\n",
    "\tlearning_rate = update_learning_rate(0)\n",
    "\n",
    "\tfor i in range(NUM_EPISODES):\n",
    "\t\tobservation = env.reset()\n",
    "\t\tstate_0 = get_Box(observation)\n",
    "\t\tfor _ in range(250):\n",
    "\t\t\tenv.render()\n",
    "\t\t\taction = update_action(state_0, explore_rate)\n",
    "\t\t\tobv, reward, done, info = env.step(action)\n",
    "\t\t\tstate_1 = get_Box(obv)\n",
    "\n",
    "\t\t\tq_max = np.max(Q[state_0])\n",
    "\t\t\tQ[state_0, action] += learning_rate*(reward + GAMMA*np.amax(Q[state_1])- Q[state_0, action])\n",
    "\t\t\tstate_0 = state_1\n",
    "\n",
    "\t\t\ttotal_reward += reward\n",
    "\n",
    "\t\t\tif done:\n",
    "\t\t\t\tif _ > 192:\n",
    "\t\t\t\t\ttotal_completions += 1\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\tlearning_rate = update_learning_rate(i)\n",
    "\t\texplore_rate = update_explore_rate(i)\n",
    "\t\tprint(\"Completions : \", total_completions)\n",
    "\t\tprint(\"REWARD/TIME: \", total_reward/(i+1))\n",
    "\t\tprint(\"Trial: \", i)\n",
    "\t\t#print(\"Final Q values: \", Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "\n",
    "\"\"\"\n",
    "lets define some constants first\n",
    "\"\"\"\n",
    "GAMMA = 0.99\n",
    "NUM_EPISODES = 2000\n",
    "MIN_EXPLORE_RATE = 0.01\n",
    "MIN_LEARNING_RATE = 0.05\n",
    "Q = np.random.rand(162, env.action_space.n)  # 162 x 2 noisey matrix\n",
    "\n",
    "\n",
    "q_learn()\n",
    "\n",
    "# note \"esc r y\"  to clear output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
